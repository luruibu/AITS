#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¯¦ç»†åˆ†æå·¥å…·
æ·±å…¥åˆ†ææ•°æ®åº“å„éƒ¨åˆ†çš„ç©ºé—´å ç”¨
"""

import sys
import os

# Windowsç¼–ç å…¼å®¹æ€§è®¾ç½®
if sys.platform.startswith('win'):
    import codecs
    try:
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        os.environ['PYTHONUTF8'] = '1'
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

import sqlite3
from pathlib import Path

def format_size(bytes_size):
    """æ ¼å¼åŒ–å­—èŠ‚å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} TB"

def analyze_database(db_path='tree_generator.db'):
    """è¯¦ç»†åˆ†ææ•°æ®åº“"""
    
    if not Path(db_path).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return
    
    # è·å–æ–‡ä»¶å¤§å°
    total_size = Path(db_path).stat().st_size
    
    print("\n" + "="*80)
    print("ğŸ” æ•°æ®åº“è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print(f"\nğŸ“ æ•°æ®åº“æ–‡ä»¶: {db_path}")
    print(f"ğŸ’¾ æ€»å¤§å°: {format_size(total_size)} ({total_size:,} bytes)")
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # 1. åˆ†æå„è¡¨çš„è®°å½•æ•°å’Œå¤§å°
    print("\n" + "="*80)
    print("ğŸ“Š è¡¨ç»“æ„åˆ†æ")
    print("="*80)
    
    # è·å–æ‰€æœ‰è¡¨
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
    tables = [row[0] for row in cursor.fetchall()]
    
    table_stats = []
    
    for table in tables:
        # è·å–è®°å½•æ•°
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        count = cursor.fetchone()[0]
        
        # ä¼°ç®—è¡¨å¤§å°ï¼ˆé€šè¿‡å®é™…æ•°æ®å¤§å°ï¼‰
        # è·å–æ‰€æœ‰åˆ—
        cursor.execute(f"PRAGMA table_info({table})")
        columns = [row[1] for row in cursor.fetchall()]
        
        # è®¡ç®—æ‰€æœ‰åˆ—çš„æ€»å¤§å°
        size_query = " + ".join([f"COALESCE(LENGTH({col}), 0)" for col in columns])
        cursor.execute(f"SELECT SUM({size_query}) FROM {table}")
        result = cursor.fetchone()
        size = result[0] if result[0] else 0
        
        table_stats.append({
            'name': table,
            'count': count,
            'size': size,
            'percent': (size / total_size * 100) if total_size > 0 else 0
        })
    
    # æŒ‰å¤§å°æ’åº
    table_stats.sort(key=lambda x: x['size'], reverse=True)
    
    print(f"\n{'è¡¨å':<30} {'è®°å½•æ•°':>10} {'å¤§å°':>15} {'å æ¯”':>10}")
    print("-" * 80)
    
    for stat in table_stats:
        print(f"{stat['name']:<30} {stat['count']:>10,} {format_size(stat['size']):>15} {stat['percent']:>9.2f}%")
    
    # 2. è¯¦ç»†åˆ†ænodesè¡¨ï¼ˆé€šå¸¸æ˜¯æœ€å¤§çš„ï¼‰
    print("\n" + "="*80)
    print("ğŸ” nodesè¡¨è¯¦ç»†åˆ†æ")
    print("="*80)
    
    # ç»Ÿè®¡æœ‰å›¾åƒæ•°æ®çš„èŠ‚ç‚¹
    cursor.execute("SELECT COUNT(*) FROM nodes WHERE image_data IS NOT NULL")
    nodes_with_images = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM nodes WHERE image_data IS NULL")
    nodes_without_images = cursor.fetchone()[0]
    
    print(f"\nèŠ‚ç‚¹æ€»æ•°: {nodes_with_images + nodes_without_images:,}")
    print(f"  - æœ‰å›¾åƒæ•°æ®: {nodes_with_images:,}")
    print(f"  - æ— å›¾åƒæ•°æ®: {nodes_without_images:,}")
    
    # è®¡ç®—å›¾åƒæ•°æ®æ€»å¤§å°
    cursor.execute("SELECT SUM(LENGTH(image_data)) FROM nodes WHERE image_data IS NOT NULL")
    result = cursor.fetchone()
    image_data_size = result[0] if result[0] else 0
    
    print(f"\nå›¾åƒæ•°æ®æ€»å¤§å°: {format_size(image_data_size)} ({image_data_size:,} bytes)")
    print(f"å›¾åƒæ•°æ®å æ¯”: {(image_data_size / total_size * 100):.2f}%")
    
    if nodes_with_images > 0:
        avg_image_size = image_data_size / nodes_with_images
        print(f"å¹³å‡æ¯å¼ å›¾åƒ: {format_size(avg_image_size)}")
    
    # è®¡ç®—å…¶ä»–å­—æ®µçš„å¤§å°
    cursor.execute("""
        SELECT 
            SUM(LENGTH(prompt)) as prompt_size,
            SUM(LENGTH(keywords)) as keywords_size,
            SUM(LENGTH(branch_info)) as branch_info_size,
            SUM(LENGTH(image_path)) as image_path_size
        FROM nodes
    """)
    result = cursor.fetchone()
    
    if result:
        prompt_size, keywords_size, branch_info_size, image_path_size = result
        prompt_size = prompt_size or 0
        keywords_size = keywords_size or 0
        branch_info_size = branch_info_size or 0
        image_path_size = image_path_size or 0
        
        print(f"\nå…¶ä»–å­—æ®µå¤§å°:")
        print(f"  - æç¤ºè¯ (prompt): {format_size(prompt_size)}")
        print(f"  - å…³é”®è¯ (keywords): {format_size(keywords_size)}")
        print(f"  - åˆ†æ”¯ä¿¡æ¯ (branch_info): {format_size(branch_info_size)}")
        print(f"  - å›¾åƒè·¯å¾„ (image_path): {format_size(image_path_size)}")
    
    # 3. åˆ†ækeyword_cacheè¡¨
    print("\n" + "="*80)
    print("ğŸ” keyword_cacheè¡¨åˆ†æ")
    print("="*80)
    
    cursor.execute("SELECT COUNT(*) FROM keyword_cache")
    cache_count = cursor.fetchone()[0]
    
    cursor.execute("SELECT SUM(LENGTH(keywords)) FROM keyword_cache")
    result = cursor.fetchone()
    cache_size = result[0] if result[0] else 0
    
    print(f"\nç¼“å­˜è®°å½•æ•°: {cache_count:,}")
    print(f"ç¼“å­˜æ•°æ®å¤§å°: {format_size(cache_size)}")
    print(f"ç¼“å­˜å æ¯”: {(cache_size / total_size * 100):.2f}%")
    
    # åˆ†æä½¿ç”¨é¢‘ç‡
    cursor.execute("""
        SELECT 
            COUNT(*) as count,
            SUM(CASE WHEN usage_count = 1 THEN 1 ELSE 0 END) as single_use,
            SUM(CASE WHEN usage_count > 1 THEN 1 ELSE 0 END) as multi_use
        FROM keyword_cache
    """)
    result = cursor.fetchone()
    if result:
        total, single, multi = result
        print(f"\nä½¿ç”¨é¢‘ç‡åˆ†æ:")
        print(f"  - ä»…ä½¿ç”¨1æ¬¡: {single:,} ({(single/total*100):.1f}%)")
        print(f"  - ä½¿ç”¨å¤šæ¬¡: {multi:,} ({(multi/total*100):.1f}%)")
    
    # 4. åˆ†ægeneration_tasksè¡¨
    print("\n" + "="*80)
    print("ğŸ” generation_tasksè¡¨åˆ†æ")
    print("="*80)
    
    cursor.execute("""
        SELECT 
            status,
            COUNT(*) as count,
            SUM(LENGTH(result)) as result_size,
            SUM(LENGTH(error_message)) as error_size
        FROM generation_tasks
        GROUP BY status
    """)
    
    print(f"\n{'çŠ¶æ€':<15} {'æ•°é‡':>10} {'ç»“æœå¤§å°':>15} {'é”™è¯¯ä¿¡æ¯å¤§å°':>15}")
    print("-" * 80)
    
    for row in cursor.fetchall():
        status, count, result_size, error_size = row
        result_size = result_size or 0
        error_size = error_size or 0
        print(f"{status:<15} {count:>10,} {format_size(result_size):>15} {format_size(error_size):>15}")
    
    # 5. æ•°æ®åº“å†…éƒ¨ç»“æ„åˆ†æ
    print("\n" + "="*80)
    print("ğŸ” æ•°æ®åº“å†…éƒ¨ç»“æ„")
    print("="*80)
    
    # è·å–é¡µé¢ç»Ÿè®¡
    cursor.execute("PRAGMA page_count")
    page_count = cursor.fetchone()[0]
    
    cursor.execute("PRAGMA page_size")
    page_size = cursor.fetchone()[0]
    
    cursor.execute("PRAGMA freelist_count")
    freelist_count = cursor.fetchone()[0]
    
    used_pages = page_count - freelist_count
    used_size = used_pages * page_size
    free_size = freelist_count * page_size
    
    print(f"\né¡µé¢ä¿¡æ¯:")
    print(f"  - é¡µé¢å¤§å°: {format_size(page_size)}")
    print(f"  - æ€»é¡µé¢æ•°: {page_count:,}")
    print(f"  - ä½¿ç”¨é¡µé¢: {used_pages:,} ({format_size(used_size)})")
    print(f"  - ç©ºé—²é¡µé¢: {freelist_count:,} ({format_size(free_size)})")
    print(f"  - ç©ºé—´åˆ©ç”¨ç‡: {(used_size / total_size * 100):.2f}%")
    
    if freelist_count > 0:
        print(f"\nğŸ’¡ æç¤º: æœ‰ {format_size(free_size)} çš„ç©ºé—²ç©ºé—´å¯ä»¥é€šè¿‡ VACUUM å›æ”¶")
    
    # 6. ç´¢å¼•åˆ†æ
    print("\n" + "="*80)
    print("ğŸ” ç´¢å¼•åˆ†æ")
    print("="*80)
    
    cursor.execute("""
        SELECT name, tbl_name 
        FROM sqlite_master 
        WHERE type='index' AND sql IS NOT NULL
        ORDER BY tbl_name, name
    """)
    
    indexes = cursor.fetchall()
    
    print(f"\nç´¢å¼•æ€»æ•°: {len(indexes)}")
    
    index_stats = []
    for idx_name, tbl_name in indexes:
        # ç®€åŒ–ï¼šç´¢å¼•å¤§å°éš¾ä»¥ç²¾ç¡®è®¡ç®—ï¼Œä½¿ç”¨ä¼°ç®—
        size = 0  # æš‚æ—¶è®¾ä¸º0ï¼Œå› ä¸ºæ— æ³•ç²¾ç¡®è®¡ç®—
        index_stats.append({
            'name': idx_name,
            'table': tbl_name,
            'size': size
        })
    
    index_stats.sort(key=lambda x: x['size'], reverse=True)
    
    total_index_size = sum(s['size'] for s in index_stats)
    
    # ä¼°ç®—ç´¢å¼•å¤§å°ä¸ºæ•°æ®å¤§å°çš„10-20%
    estimated_index_size = int(total_size * 0.15)
    
    print(f"ç´¢å¼•ä¼°ç®—å¤§å°: {format_size(estimated_index_size)} (çº¦å  15%)")
    
    if len(index_stats) > 0:
        print(f"\nç´¢å¼•åˆ—è¡¨:")
        print(f"{'ç´¢å¼•å':<40} {'è¡¨å':<20}")
        print("-" * 80)
        
        for stat in index_stats[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"{stat['name']:<40} {stat['table']:<20}")
    
    # 7. æ€»ç»“å’Œå»ºè®®
    print("\n" + "="*80)
    print("ğŸ’¡ åˆ†ææ€»ç»“å’Œå»ºè®®")
    print("="*80)
    
    print("\nğŸ“Š ç©ºé—´å ç”¨æ’å:")
    
    # è®¡ç®—å…¶ä»–å¼€é”€ï¼ˆæ•°æ®åº“å…ƒæ•°æ®ã€ç´¢å¼•ç­‰ï¼‰
    accounted_size = image_data_size + cache_size + free_size
    other_overhead = total_size - accounted_size
    
    components = [
        ('å›¾åƒæ•°æ®', image_data_size),
        ('æ•°æ®åº“å¼€é”€(ç´¢å¼•/å…ƒæ•°æ®ç­‰)', other_overhead),
        ('å…³é”®è¯ç¼“å­˜', cache_size),
        ('ç©ºé—²ç©ºé—´', free_size),
    ]
    
    components.sort(key=lambda x: x[1], reverse=True)
    
    for i, (name, size) in enumerate(components, 1):
        percent = (size / total_size * 100) if total_size > 0 else 0
        print(f"{i}. {name}: {format_size(size)} ({percent:.2f}%)")
    
    print("\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
    
    suggestions = []
    
    if image_data_size > total_size * 0.5:
        suggestions.append("âš ï¸ å›¾åƒæ•°æ®å æ¯”è¶…è¿‡50%ï¼Œå»ºè®®æ¸…ç†æ—§å›¾åƒæ•°æ®")
        suggestions.append("   å‘½ä»¤: python db_maintenance.py --cleanup-images 7")
    
    if freelist_count > page_count * 0.1:
        suggestions.append("âš ï¸ ç©ºé—²ç©ºé—´è¶…è¿‡10%ï¼Œå»ºè®®æ‰§è¡ŒVACUUMä¼˜åŒ–")
        suggestions.append("   å‘½ä»¤: python db_maintenance.py --vacuum")
    
    if cache_count > 1000:
        cursor.execute("SELECT COUNT(*) FROM keyword_cache WHERE usage_count = 1")
        single_use = cursor.fetchone()[0]
        if single_use > cache_count * 0.5:
            suggestions.append("âš ï¸ è¶…è¿‡50%çš„ç¼“å­˜ä»…ä½¿ç”¨1æ¬¡ï¼Œå»ºè®®æ¸…ç†")
            suggestions.append("   å‘½ä»¤: python db_maintenance.py --cleanup-old 30")
    
    cursor.execute("SELECT COUNT(*) FROM generation_tasks WHERE status = 'failed'")
    failed_tasks = cursor.fetchone()[0]
    if failed_tasks > 0:
        suggestions.append(f"âš ï¸ æœ‰ {failed_tasks} ä¸ªå¤±è´¥ä»»åŠ¡ï¼Œå»ºè®®æ¸…ç†")
        suggestions.append("   å‘½ä»¤: python db_maintenance.py --cleanup-failed")
    
    if not suggestions:
        suggestions.append("âœ… æ•°æ®åº“çŠ¶æ€è‰¯å¥½ï¼Œæš‚æ— ä¼˜åŒ–å»ºè®®")
    
    for suggestion in suggestions:
        print(suggestion)
    
    # 8. é¢„æœŸä¼˜åŒ–æ•ˆæœ
    if image_data_size > 0 or freelist_count > 0:
        print("\nğŸ“ˆ é¢„æœŸä¼˜åŒ–æ•ˆæœ:")
        
        potential_savings = 0
        
        if image_data_size > total_size * 0.5:
            # å‡è®¾æ¸…ç†70%çš„å›¾åƒæ•°æ®
            image_savings = image_data_size * 0.7
            potential_savings += image_savings
            print(f"  - æ¸…ç†å›¾åƒæ•°æ®: å¯èŠ‚çœçº¦ {format_size(image_savings)}")
        
        if freelist_count > 0:
            potential_savings += free_size
            print(f"  - VACUUMä¼˜åŒ–: å¯èŠ‚çœçº¦ {format_size(free_size)}")
        
        if potential_savings > 0:
            final_size = total_size - potential_savings
            print(f"\n  æ€»è®¡å¯èŠ‚çœ: {format_size(potential_savings)} ({(potential_savings / total_size * 100):.1f}%)")
            print(f"  ä¼˜åŒ–åå¤§å°: {format_size(final_size)}")
    
    conn.close()
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆ")
    print("="*80 + "\n")

if __name__ == '__main__':
    analyze_database()
